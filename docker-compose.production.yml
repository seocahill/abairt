version: '3.8'

services:
  web:
    image: caddy:alpine
    ports:
      - "80:80"
      - "443:443"
    configs:
      - source: caddyfile_v7
        target: /etc/caddy/Caddyfile
    volumes:
      - caddy_data:/data
      - caddy_config:/config
      # - assets:/var/www/site/current/public

  rails:
    image: registry.gitlab.com/abairt/web-application:5bcf8c2c0b5a9c408a4ff70d017079d742d471d7
    deploy:
      mode: replicated
      replicas: 1
    environment:
      REDIS_URL: redis://redis:6379
      SELENIUM_URL: http://chrome:4444/wd/hub
      RAILS_SERVE_STATIC_FILES: "true"
      RAILS_LOG_TO_STDOUT: "true"
      BUNDLE_PATH: "/usr/local/bundle"
      BUNDLE_WITHOUT: "development:test"
      SELENIUM_HEADFUL: "true"
    secrets:
      - source: master_key
        target: /rails/config/master.key
    volumes:
      - db:/rails/db
      # - assets:/rails/public
    command:
      - /bin/bash
      - -c
      - |
        bin/rails db:create db:migrate
        bin/rails s -p 3000 -b 0.0.0.0

  # litestream:
  #   image: litestream/litestream
  #   deploy:
  #     replicas: 1
  #   secrets:
  #     - source: litestream_v5
  #       target: /etc/litestream.yml
  #   volumes:
  #     - db:/data
  #   user: "1000:1000"
  #   entrypoint:
  #     - /bin/sh
  #     - -c
  #     - |
  #       litestream restore -if-db-not-exists /data/production.sqlite3
  #       litestream replicate

  backup:
    image: amazon/aws-cli:latest
    volumes:
      - db:/data:ro
    environment:
      AWS_ACCESS_KEY_ID: AKIAQLYSUE62IMPOF7U4
      AWS_SECRET_ACCESS_KEY: 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
      AWS_DEFAULT_REGION: eu-west-1
    entrypoint:
      - /bin/sh
      - -c
      - |
        # Install dependencies
        yum install -y tar gzip findutils

        # Backup function
        backup() {
          BACKUP_DATE=$$(date +%Y%m%d-%H%M)
          BACKUP_DIR="/tmp/db-backup"
          S3_BUCKET="s3://abairt-db/backups"

          mkdir -p $$BACKUP_DIR

          # Copy database files
          cp /data/production.sqlite3 $$BACKUP_DIR/ 2>/dev/null || true
          cp /data/production.sqlite3-wal $$BACKUP_DIR/ 2>/dev/null || true
          cp /data/production.sqlite3-shm $$BACKUP_DIR/ 2>/dev/null || true

          # Create tarball
          cd $$BACKUP_DIR
          tar czf /tmp/abairt-db-backup-$$BACKUP_DATE.tar.gz production.sqlite3*

          # Upload to S3
          aws s3 cp /tmp/abairt-db-backup-$$BACKUP_DATE.tar.gz $$S3_BUCKET/abairt-db-backup-$$BACKUP_DATE.tar.gz

          # Cleanup
          rm -rf $$BACKUP_DIR /tmp/abairt-db-backup-$$BACKUP_DATE.tar.gz

          echo "$$(date): Backup completed - abairt-db-backup-$$BACKUP_DATE.tar.gz"
        }

        # Run backup every 6 hours
        while true; do
          backup
          sleep 21600  # 6 hours
        done

  redis:
    image: redis:alpine

  chrome:
    image: selenium/standalone-chrome:4.17.0
    deploy:
      mode: replicated
      replicas: 1
      resources:
        limits:
          memory: 2G
        reservations:
          memory: 1G
      placement:
        constraints:
          - node.role == manager
    ports:
      - "7900:7900"  # noVNC web interface for visual debugging
      - "5900:5900"  # VNC port (for VNC clients)
    tmpfs:
      - /dev/shm:rw,noexec,nosuid,size=2g
    environment:
      - SE_NODE_MAX_SESSIONS=5
      - SE_NODE_SESSION_TIMEOUT=300
      - SE_VNC_NO_PASSWORD=1  # Disable VNC password for easier access


volumes:
  db:
  vector_db:
  caddy_data:
  caddy_config:
  # assets:

configs:
  caddyfile_v7:
    file: ./Caddyfile

secrets:
  litestream_v5:
    file: ./litestream.yml
  master_key:
    file: ./config/master.key